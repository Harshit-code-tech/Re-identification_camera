{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12279218,"sourceType":"datasetVersion","datasetId":7738247},{"sourceId":448833,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":364398,"modelId":385278}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Jupyter Notebook: Player Tracking and Re-Identification in a Single Video Feed","metadata":{}},{"cell_type":"markdown","source":"## Overview","metadata":{}},{"cell_type":"markdown","source":"\nThis notebook implements a player tracking and re-identification pipeline for a 15-second video clip. The goal is to detect players using a fine-tuned YOLOv11 model, track them using the Norfair library, and maintain consistent player identities across frames, even when players exit and re-enter the frame. The pipeline includes:\n\n* Object Detection: Using YOLOv11 to detect players.\n* Tracking: Using Norfair to assign tracking IDs.\n* Re-Identification: Using histogram-based appearance matching to maintain consistent IDs.\n* Visualization: Saving tracked and re-identified outputs as videos and logging ID switches.\n* Analysis: Plotting histogram similarity scores and logging ID changes.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Installation","metadata":{}},{"cell_type":"code","source":"!pip install ultralytics opencv-python-headless norfair matplotlib pandas --quiet","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport random\nfrom collections import defaultdict\nfrom ultralytics import YOLO\nfrom norfair import Tracker, Detection\nimport matplotlib.pyplot as plt\nimport pandas as pd","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"# Video paths\nvideo_path = '/kaggle/input/15-sec-video/15_sec_video.mp4'\nreid_output_path = '/kaggle/working/reid_output.mp4'\ntrack_output_path = '/kaggle/working/tracked_output_with_head.mp4'\n\n# Model path\nmodel_path = '/kaggle/input/best/pytorch/default/1/best.pt'\n\n# Tracker parameters\ndistance_threshold = 150\ninitialization_delay = 1\nnum_reference_frames = 50  # Frames to build reference gallery\nmatch_threshold = 0.85  # Histogram similarity threshold for re-identification","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Model and Video","metadata":{}},{"cell_type":"code","source":"# Load YOLOv11 model\nmodel = YOLO(model_path)\nprint(\"Model class names:\", model.names)\n\n# Load video\ncap = cv2.VideoCapture(video_path)\nif not cap.isOpened():\n    print(\"Error: Could not open video file.\")\n    exit(1)\n\n# Video properties\nfps = cap.get(cv2.CAP_PROP_FPS)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nprint(f\"Video properties: FPS={fps}, Width={width}, Height={height}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Initialize Video Writers","metadata":{}},{"cell_type":"code","source":"reid_out = cv2.VideoWriter(reid_output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\ntrack_out = cv2.VideoWriter(track_output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\nif not reid_out.isOpened() or not track_out.isOpened():\n    print(\"Error: Could not initialize video writer.\")\n    cap.release()\n    exit(1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tracker Setup","metadata":{}},{"cell_type":"code","source":"def euclidean_distance(detection, tracked_object):\n    detection_center = np.array(detection.points.mean(axis=0))\n    tracked_center = np.array(tracked_object.estimate.mean(axis=0))\n    return np.linalg.norm(detection_center - tracked_center)\n\ntracker = Tracker(\n    distance_function=euclidean_distance,\n    distance_threshold=distance_threshold,\n    initialization_delay=initialization_delay,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Re-Identification Setup","metadata":{}},{"cell_type":"code","source":"# Reference gallery for re-identification\nreference_gallery = {}\nid_colors = defaultdict(lambda: (random.randint(50, 255), random.randint(50, 255), random.randint(50, 255)))\nid_remap = {}  # Maps tracker IDs to re-identified IDs\nid_switch_log = []  # Log ID switches: (frame, sid, old_id, new_id, score)\nprev_assignments = {}  # Track previous ID assignments\nall_scores = []  # Store all histogram similarity scores","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Histogram Functions","metadata":{}},{"cell_type":"code","source":"def compute_histogram(img):\n    if img.size == 0:\n        return None\n    try:\n        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n        hist = cv2.calcHist([hsv], [0, 1], None, [50, 60], [0, 180, 0, 256])\n        cv2.normalize(hist, hist)\n        return hist\n    except Exception as e:\n        print(f\"Error computing histogram: {e}\")\n        return None\n\ndef compare_histograms(hist1, hist2):\n    if hist1 is None or hist2 is None:\n        return 0\n    try:\n        return cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)\n    except Exception as e:\n        print(f\"Error comparing histograms: {e}\")\n        return 0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Main Processing Loop","metadata":{}},{"cell_type":"code","source":"frame_num = 0\nplayer_class_id = model.names.index('player') if 'player' in model.names else 0\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Create a copy to avoid visualization bugs\n    frame = frame.copy()\n    track_frame = frame.copy()  # For tracking-only output\n\n    # YOLO detection\n    try:\n        results = model(frame, conf=0.3)[0]\n    except Exception as e:\n        print(f\"Error in YOLO detection at frame {frame_num}: {e}\")\n        continue\n\n    # Convert detections to Norfair format\n    dets = []\n    for box in results.boxes:\n        cls_id = int(box.cls.item())\n        if cls_id == player_class_id:\n            x1, y1, x2, y2 = map(int, box.xyxy[0])\n            conf = box.conf.item()\n            points = np.array([[x1, y1], [x2, y2]])\n            dets.append(Detection(points=points, scores=np.array([conf])))\n\n    # Filter to keep only the most confident detection (single player)\n    if len(dets) > 1:\n        dets = [max(dets, key=lambda x: x.scores[0])]\n    print(f\"Frame {frame_num}: YOLO detected {len(dets)} players\")\n\n    # Update tracker\n    try:\n        tracked_objects = tracker.update(detections=dets)\n        print(f\"Frame {frame_num}: Norfair tracks {len(tracked_objects)} objects\")\n    except Exception as e:\n        print(f\"Error in Norfair tracker at frame {frame_num}: {e}\")\n        continue\n\n    # Build reference gallery (first 50 frames)\n    if frame_num < num_reference_frames:\n        for obj in tracked_objects:\n            tid = obj.id\n            if tid not in reference_gallery:\n                x1, y1 = map(int, obj.estimate[0])\n                x2, y2 = map(int, obj.estimate[1])\n                crop = frame[y1:y2, x1:x2]\n                if crop.size != 0:\n                    reference_gallery[tid] = crop\n\n    # Re-identification for later frames\n    else:\n        for obj in tracked_objects:\n            sid = obj.id\n            x1, y1 = map(int, obj.estimate[0])\n            x2, y2 = map(int, obj.estimate[1])\n            crop = frame[y1:y2, x1:x2]\n            if crop.size == 0:\n                continue\n\n            hist_new = compute_histogram(crop)\n            best_id, best_score = None, 0\n\n            for gallery_id, ref_crop in reference_gallery.items():\n                hist_ref = compute_histogram(ref_crop)\n                score = compare_histograms(hist_ref, hist_new)\n                all_scores.append(score)\n                if score > best_score and score > match_threshold:\n                    best_score = score\n                    best_id = gallery_id\n\n            new_real_id = best_id if best_id is not None else 1  # Default to ID 1 for single player\n            if sid in prev_assignments and prev_assignments[sid] != new_real_id:\n                id_switch_log.append((frame_num, sid, prev_assignments[sid], new_real_id, best_score))\n            prev_assignments[sid] = new_real_id\n            id_remap[sid] = new_real_id\n\n    # Draw annotations for tracking output\n    for obj in tracked_objects:\n        x1, y1 = map(int, obj.estimate[0])\n        x2, y2 = map(int, obj.estimate[1])\n        sid = obj.id\n        color = id_colors[sid]\n\n        # Bounding box and ID\n        cv2.rectangle(track_frame, (x1, y1), (x2, y2), color, 2)\n        label = f\"ID {sid}\"\n        (text_w, text_h), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n        cv2.rectangle(track_frame, (x1, y1 - text_h - baseline - 4), (x1 + text_w, y1), (0, 0, 0), -1)\n        cv2.putText(track_frame, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n\n        # Head region\n        head_y2 = y1 + (y2 - y1) // 4\n        cv2.rectangle(track_frame, (x1, y1), (x2, head_y2), (0, 0, 255), 1)\n\n    # Draw annotations for re-identification output\n    for obj in tracked_objects:\n        x1, y1 = map(int, obj.estimate[0])\n        x2, y2 = map(int, obj.estimate[1])\n        sid = obj.id\n        real_id = id_remap.get(sid, sid) if frame_num >= num_reference_frames else sid\n        color = id_colors[real_id]\n\n        # Bounding box and ID\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n        label = f\"ID {real_id}\"\n        (text_w, text_h), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n        cv2.rectangle(frame, (x1, y1 - text_h - baseline - 4), (x1 + text_w, y1), (0, 0, 0), -1)\n        cv2.putText(frame, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n\n        # Head region\n        head_y2 = y1 + (y2 - y1) // 4\n        cv2.rectangle(frame, (x1, y1), (x2, head_y2), (0, 0, 255), 1)\n\n\n    # Add frame info\n    cv2.putText(frame, f\"Frame: {frame_num}\", (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n    cv2.putText(frame, f\"FPS: {int(fps)}\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n    cv2.putText(track_frame, f\"Frame: {frame_num}\", (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n    cv2.putText(track_frame, f\"FPS: {int(fps)}\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n\n    # Write frames\n    reid_out.write(frame)\n    track_out.write(track_frame)\n\n    # Save sample frame\n    if frame_num % 30 == 0:\n        cv2.imwrite(f'/kaggle/working/sample_frame_{frame_num}.jpg', frame)\n        print(f\"Frame {frame_num}: Saved sample frame\")\n\n    frame_num += 1\n\n# Cleanup\ncap.release()\nreid_out.release()\ntrack_out.release()\nprint(f\"ðŸŽ¥ Tracking video saved at: {track_output_path}\")\nprint(f\"ðŸŽ¥ Re-ID video saved at: {reid_output_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Analysis and Visualization","metadata":{}},{"cell_type":"code","source":"# Save ID switch log\nswitch_df = pd.DataFrame(id_switch_log, columns=[\"Frame\", \"SID\", \"Old_ID\", \"New_ID\", \"Score\"])\nswitch_df.to_csv(\"/kaggle/working/id_switch_log.csv\", index=False)\nprint(\"âœ… ID switch log saved\")\n\n# Plot scores\nplt.figure(figsize=(10, 5))\nplt.plot(all_scores, color=\"blue\", label=\"Histogram Match Score\")\nplt.axhline(match_threshold, color=\"red\", linestyle=\"--\", label=f\"Threshold ({match_threshold})\")\nplt.title(\"Histogram Similarity Scores Over Time\")\nplt.xlabel(\"Comparison #\")\nplt.ylabel(\"Correlation Score\")\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.savefig(\"/kaggle/working/reid_scores_plot.png\")\nplt.close()\nprint(\"âœ… Re-ID score plot saved\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}